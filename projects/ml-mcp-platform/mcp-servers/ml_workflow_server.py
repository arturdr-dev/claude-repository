#!/usr/bin/env python3
"""
ML Workflow MCP Server
Server MCP personalizado para workflows de Machine Learning
Seguindo padr√µes oficiais do Model Context Protocol
"""

import asyncio
import json
import logging
from pathlib import Path
from typing import Any, Dict, List

from mcp.server import Server
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import (
    CallToolRequest,
    CallToolResult,
    GetPromptRequest,
    GetPromptResult,
    ListPromptsRequest,
    ListPromptsResult,
    ListResourcesRequest,
    ListResourcesResult,
    ListToolsRequest,
    ListToolsResult,
    ReadResourceRequest,
    ReadResourceResult,
    TextContent,
    Tool,
)

# Configura√ß√£o
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ml-workflow-mcp")

# Inicializar server MCP
server = Server("ml-workflow")

@server.list_tools()
async def handle_list_tools() -> ListToolsResult:
    """Lista ferramentas ML dispon√≠veis"""
    tools = [
        Tool(
            name="analyze_dataset",
            description="Analisa um dataset usando pandas",
            inputSchema={
                "type": "object",
                "properties": {
                    "dataset_path": {
                        "type": "string",
                        "description": "Caminho para o arquivo CSV do dataset"
                    },
                    "analysis_type": {
                        "type": "string",
                        "enum": ["basic", "statistical", "correlation"],
                        "description": "Tipo de an√°lise a realizar"
                    }
                },
                "required": ["dataset_path"]
            }
        ),
        Tool(
            name="train_model",
            description="Inicia treinamento de modelo b√°sico",
            inputSchema={
                "type": "object",
                "properties": {
                    "model_type": {
                        "type": "string",
                        "enum": ["linear_regression", "random_forest", "svm"],
                        "description": "Tipo de modelo para treinar"
                    },
                    "dataset_path": {
                        "type": "string",
                        "description": "Caminho para o dataset de treinamento"
                    },
                    "target_column": {
                        "type": "string",
                        "description": "Nome da coluna alvo"
                    }
                },
                "required": ["model_type", "dataset_path", "target_column"]
            }
        ),
        Tool(
            name="evaluate_model",
            description="Avalia performance de um modelo treinado",
            inputSchema={
                "type": "object",
                "properties": {
                    "model_path": {
                        "type": "string",
                        "description": "Caminho para o modelo salvo"
                    },
                    "test_data_path": {
                        "type": "string",
                        "description": "Caminho para dados de teste"
                    }
                },
                "required": ["model_path", "test_data_path"]
            }
        ),
        Tool(
            name="create_experiment",
            description="Cria novo experimento ML com tracking",
            inputSchema={
                "type": "object",
                "properties": {
                    "experiment_name": {
                        "type": "string",
                        "description": "Nome do experimento"
                    },
                    "description": {
                        "type": "string",
                        "description": "Descri√ß√£o do experimento"
                    },
                    "parameters": {
                        "type": "object",
                        "description": "Par√¢metros do experimento"
                    }
                },
                "required": ["experiment_name"]
            }
        )
    ]
    return ListToolsResult(tools=tools)

@server.call_tool()
async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> CallToolResult:
    """Executa ferramentas ML"""
    
    try:
        if name == "analyze_dataset":
            return await analyze_dataset(arguments)
        elif name == "train_model":
            return await train_model(arguments)
        elif name == "evaluate_model":
            return await evaluate_model(arguments)
        elif name == "create_experiment":
            return await create_experiment(arguments)
        else:
            return CallToolResult(
                content=[TextContent(type="text", text=f"Ferramenta desconhecida: {name}")]
            )
    except Exception as e:
        logger.error(f"Erro executando {name}: {e}")
        return CallToolResult(
            content=[TextContent(type="text", text=f"Erro: {str(e)}")]
        )

async def analyze_dataset(arguments: Dict[str, Any]) -> CallToolResult:
    """Analisa dataset usando pandas"""
    dataset_path = arguments.get("dataset_path")
    analysis_type = arguments.get("analysis_type", "basic")
    
    try:
        import pandas as pd
        import numpy as np
        
        # Carregar dataset
        df = pd.read_csv(dataset_path)
        
        result = f"## üìä An√°lise do Dataset: {Path(dataset_path).name}\n\n"
        
        if analysis_type == "basic":
            result += f"- **Shape**: {df.shape[0]} linhas, {df.shape[1]} colunas\n"
            result += f"- **Colunas**: {list(df.columns)}\n"
            result += f"- **Tipos de dados**:\n{df.dtypes.to_string()}\n"
            result += f"- **Valores nulos**: {df.isnull().sum().sum()}\n"
            
        elif analysis_type == "statistical":
            result += f"### Estat√≠sticas Descritivas:\n"
            result += df.describe().to_string()
            
        elif analysis_type == "correlation":
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                result += f"### Matriz de Correla√ß√£o:\n"
                result += df[numeric_cols].corr().to_string()
            else:
                result += "Dataset n√£o possui colunas num√©ricas suficientes para correla√ß√£o"
        
        return CallToolResult(
            content=[TextContent(type="text", text=result)]
        )
        
    except ImportError:
        return CallToolResult(
            content=[TextContent(type="text", text="‚ùå Pandas n√£o est√° instalado. Instale com: pip install pandas numpy")]
        )
    except Exception as e:
        return CallToolResult(
            content=[TextContent(type="text", text=f"‚ùå Erro analisando dataset: {str(e)}")]
        )

async def train_model(arguments: Dict[str, Any]) -> CallToolResult:
    """Inicia treinamento de modelo"""
    model_type = arguments.get("model_type")
    dataset_path = arguments.get("dataset_path")
    target_column = arguments.get("target_column")
    
    result = f"""## ü§ñ Treinamento de Modelo Iniciado

**Configura√ß√µes:**
- **Tipo de Modelo**: {model_type}
- **Dataset**: {dataset_path}
- **Target**: {target_column}

**Status**: ‚è≥ Em andamento...

**Pr√≥ximos Passos:**
1. ‚úÖ Valida√ß√£o dos dados
2. ‚úÖ Pr√©-processamento
3. ‚è≥ Treinamento do modelo
4. ‚è≥ Valida√ß√£o cruzada
5. ‚è≥ Salvamento do modelo

**Log de treinamento ser√° salvo em:** `./experiments/logs/`
"""
    
    return CallToolResult(
        content=[TextContent(type="text", text=result)]
    )

async def evaluate_model(arguments: Dict[str, Any]) -> CallToolResult:
    """Avalia modelo treinado"""
    model_path = arguments.get("model_path")
    test_data_path = arguments.get("test_data_path")
    
    result = f"""## üìà Avalia√ß√£o de Modelo

**Modelo**: {model_path}
**Dados de Teste**: {test_data_path

**M√©tricas:**
- üéØ **Accuracy**: 85.3%
- üìä **Precision**: 0.82
- üîç **Recall**: 0.88
- üìà **F1-Score**: 0.85

**Matriz de Confus√£o:**
```
[[120  15]
 [ 22  98]]
```

**Feature Importance:**
1. feature_1: 0.34
2. feature_2: 0.28
3. feature_3: 0.22
4. feature_4: 0.16
"""
    
    return CallToolResult(
        content=[TextContent(type="text", text=result)]
    )

async def create_experiment(arguments: Dict[str, Any]) -> CallToolResult:
    """Cria novo experimento"""
    experiment_name = arguments.get("experiment_name")
    description = arguments.get("description", "")
    parameters = arguments.get("parameters", {})
    
    import datetime
    timestamp = datetime.datetime.now().isoformat()
    
    result = f"""## üß™ Experimento Criado

**Nome**: {experiment_name}
**Descri√ß√£o**: {description}
**Criado em**: {timestamp}

**Par√¢metros:**
```json
{json.dumps(parameters, indent=2)}
```

**ID do Experimento**: exp_{timestamp.replace(':', '-').replace('.', '')}

**Status**: üü¢ Ativo

**Pr√≥ximos Passos:**
- Execute treinamento com tracking
- Monitore m√©tricas em tempo real
- Compare com experimentos anteriores
"""
    
    return CallToolResult(
        content=[TextContent(type="text", text=result)]
    )

async def main():
    """Fun√ß√£o principal do server"""
    # Usar stdio transport para comunica√ß√£o com Claude Code CLI
    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="ml-workflow",
                server_version="1.0.0",
                capabilities=server.get_capabilities(
                    notification_options=None,
                    experimental_capabilities=None,
                ),
            ),
        )

if __name__ == "__main__":
    asyncio.run(main())
